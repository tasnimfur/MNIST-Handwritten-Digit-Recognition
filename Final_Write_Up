Abstract
 
In recent years, the fields of computer vision and machine learning have gained more popularity. Today, computer vision applications are found in almost every 
industry. Some examples of these applications include automated cars, face recognition, and tumor detection. The objective of this project is to implement a 
program that can recognize and classify handwritten digits with an accuracy greater than 99.15% using Python and keras machine learning library. To achieve this,
a convolutional neural network (CNN) model was designed and trained using the MNIST handwritten digits dataset. After 10 epochs, the trained model achieved 
99.4% accuracy on the validation set. 

Introduction
 
Convolutional neural networks or CNNs are widely used for image classification tasks because they produce excellent results with these kinds of problems. 
A CNN consists of multiple layers and each layer processes its input in some way. The first layer of the network takes an image as input and the output is 
a set of probabilities for each possible class (i.e classes are the digits from 0 to 9 in this case). The input image is assigned the label with the highest 
probability. 

The MNIST dataset which contains 70,000 images of handwritten digits was used to train the CNN model. Each image in the dataset is a grayscale image of size 
28x28 pixels. Furthermore, the performance of the machine learning model was evaluated by calculating the accuracy score. The dataset was split into 60,000 
training examples and 10,000 test examples. Moreover, the training samples were used to train the CNN classification model whereas the test samples were used 
for measuring the test accuracy of the model. 

Background

Handwritten digit recognition is a well-known machine learning image classification problem in the field of computer science. By using CNNs, it was possible to 
solve this problem, and many people interested in this field were able to achieve accuracies greater than 99%. 

Approach

The architecture of the proposed CNN model consists of 3 convolution layers, 2 max pooling layers, and 3 dense layers. In each convolution layer, features were 
extracted through the convolution operation using a filter of size 3x3. Every convolution layer was followed by a RELU activation function, which is 
mathematically defined as y = max(0, x). An activation function introduces non-linearity into the neural network model in order for the network to learn 
feature representations more effectively (Banerjee et al, 2019). Moreover, max pooling layers were added after the second and third convolution layers in 
order to reduce the spatial dimensions and increase the number of feature maps (Hossain, Mohon, 2019).  Furthermore, to prevent overfitting, a dropout layer 
was added to the model after the first dense layer. 

Results

The model achieved 99.4% accuracy on the validation set after 10 epochs and batches of size 128. Demonstrated below is the output after running the program.

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 26, 26, 16)        160       
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 24, 24, 32)        4640      
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1600)              0         
_________________________________________________________________
dense (Dense)                (None, 128)               204928    
_________________________________________________________________
dropout (Dropout)            (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                4128      
_________________________________________________________________
dense_2 (Dense)              (None, 10)                330       
=================================================================
Total params: 232,682
Trainable params: 232,682
Non-trainable params: 0
_________________________________________________________________
2021-04-17 19:02:22.268619: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/10
469/469 [==============================] - 53s 112ms/step - loss: 0.7209 - accuracy: 0.7596 - val_loss: 0.0569 - val_accuracy: 0.9819
Epoch 2/10
469/469 [==============================] - 56s 120ms/step - loss: 0.1030 - accuracy: 0.9694 - val_loss: 0.0390 - val_accuracy: 0.9880
Epoch 3/10
469/469 [==============================] - 79s 168ms/step - loss: 0.0692 - accuracy: 0.9802 - val_loss: 0.0307 - val_accuracy: 0.9901
Epoch 4/10
469/469 [==============================] - 54s 116ms/step - loss: 0.0592 - accuracy: 0.9828 - val_loss: 0.0265 - val_accuracy: 0.9908
Epoch 5/10
469/469 [==============================] - 51s 110ms/step - loss: 0.0492 - accuracy: 0.9849 - val_loss: 0.0252 - val_accuracy: 0.9917
Epoch 6/10
469/469 [==============================] - 50s 107ms/step - loss: 0.0382 - accuracy: 0.9884 - val_loss: 0.0212 - val_accuracy: 0.9939
Epoch 7/10
469/469 [==============================] - 50s 106ms/step - loss: 0.0332 - accuracy: 0.9898 - val_loss: 0.0217 - val_accuracy: 0.9931
Epoch 8/10
469/469 [==============================] - 54s 116ms/step - loss: 0.0287 - accuracy: 0.9909 - val_loss: 0.0219 - val_accuracy: 0.9935
Epoch 9/10
469/469 [==============================] - 51s 109ms/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.0266 - val_accuracy: 0.9923
Epoch 10/10
469/469 [==============================] - 52s 112ms/step - loss: 0.0220 - accuracy: 0.9928 - val_loss: 0.0208 - val_accuracy: 0.9938
313/313 - 2s - loss: 0.0208 - accuracy: 0.9938
Test loss: 0.020815901458263397
Test accuracy: 0.9937999844551086

References

Hossain, Anwar, Mohon, Ali. 2019. Recognition of Handwritten Digit using Convolutional Neural Network (CNN). Global Journal of Computer Science and Technology: 
D Neural & Artificial Intelligence. Available at: https://core.ac.uk/download/pdf/231148505.pdf

Banerjee, Chaity, Murkherjee, Tathagata & Pasiliao, Eduardo. 2019. An Empirical Study on Generalizations of the ReLU Activation Function. Conference: the 2019 
ACM Southeast Conference. Available at: <https://www.researchgate.net/publication/332925677_An_Empirical_Study_on_Generalizations_of_the_ReLU_Activation_Function>
